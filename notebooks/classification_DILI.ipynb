{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "CUDARuntimeError",
     "evalue": "cudaErrorNoDevice: no CUDA-capable device is detected",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCUDARuntimeError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e4776bbb3563>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefer_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_core_sci_md\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ner'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'parser'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#device = torch.device('cuda:0')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/thinc/util.py\u001b[0m in \u001b[0;36mprefer_gpu\u001b[0;34m(gpu_id)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mrequire_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgpu_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/thinc/util.py\u001b[0m in \u001b[0;36mrequire_gpu\u001b[0;34m(gpu_id)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0mset_current_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCupyOps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0mset_active_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/thinc/util.py\u001b[0m in \u001b[0;36mset_active_gpu\u001b[0;34m(gpu_id)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mcupy/cuda/device.pyx\u001b[0m in \u001b[0;36mcupy.cuda.device.Device.use\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/cuda/device.pyx\u001b[0m in \u001b[0;36mcupy.cuda.device.Device.use\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy_backends/cuda/api/runtime.pyx\u001b[0m in \u001b[0;36mcupy_backends.cuda.api.runtime.setDevice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy_backends/cuda/api/runtime.pyx\u001b[0m in \u001b[0;36mcupy_backends.cuda.api.runtime.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCUDARuntimeError\u001b[0m: cudaErrorNoDevice: no CUDA-capable device is detected"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "PATH = '/home/arsentii/prog/camda/'\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"en_core_sci_md\", disable=['ner', 'parser'])\n",
    "#device = torch.device('cuda:0')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufv7olhM0USv"
   },
   "source": [
    "# Reading data, dividing by train and test sets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_yw4Fj1N0USy"
   },
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv(PATH + 'DILI_data.csv')\n",
    "validation_data = pd.read_csv(PATH + \"Validation.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FaRj3x9ydGvr"
   },
   "outputs": [],
   "source": [
    "def print_example(train_data, N=5, cols=[\"Title\", \"Label\"]):\n",
    "    frac = N / train_data.shape[0]\n",
    "    sample_data = train_data.sample( frac=frac)\n",
    "    for idx, row in sample_data.iterrows():\n",
    "        cmd = input()\n",
    "        if cmd == \"s\":\n",
    "            break\n",
    "        print( \"%s | %s >> \" %  (row['Title'], row[\"Label\"]))\n",
    "        cmd = input()\n",
    "        if cmd == \"a\":\n",
    "            if type(row['Abstract']) == str:\n",
    "                abstract = row['Abstract']\n",
    "                abstract = abstract.replace('. ', '.\\n') \n",
    "                print(abstract)\n",
    "        else:\n",
    "            continue\n",
    "def keyword_searcher(string, keyword):\n",
    "    matches = re.search(keyword, string)\n",
    "    return matches\n",
    "\n",
    "def keyword_classifier(train_data, keywords):\n",
    "    results = np.zeros((train_data.shape[0]))\n",
    "    targets = train_data[\"Label\"].values\n",
    "    keyword = '|'.join(keywords)\n",
    "    print(keyword)\n",
    "    for idx, row in train_data.iterrows():\n",
    "        title = re.sub(\"[\\?\\+\\*\\)\\(\\[\\]]\", \"\", str(row[\"Title\"]) + str(row[\"Abstract\"]))\n",
    "        matches = keyword_searcher(title, keyword)\n",
    "        if matches:\n",
    "            results[idx] = 1\n",
    "    precision = np.sum( np.logical_and(results, targets) ) / np.sum(targets)\n",
    "    accuracy = (results == targets).mean()\n",
    "    return (accuracy, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MrJw0YMFeEWI"
   },
   "outputs": [],
   "source": [
    "print_example(train_data, N=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJAJPVytJsb2"
   },
   "source": [
    "# Point mutual information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUAI2RUE0UTs"
   },
   "source": [
    "# PMI calculation\n",
    "x - class, y - word\n",
    "$$\n",
    "pmi(x, y) = log \\frac{p(x, y)}{p(x) \\cdot p(y)}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "p(x, y) = \\frac{f(y, x)}{N_x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_VJgD9PqNXpM"
   },
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    doc = nlp.make_doc(string)\n",
    "    words = [token.text.lower() for token in doc if token.is_alpha and not token.is_stop and len(token.text) > 1 ]\n",
    "    return words\n",
    "\n",
    "def tokenization(train_data):\n",
    "    tokenized_texts = []\n",
    "    for _, row in train_data.iterrows():\n",
    "        text = row['Title'] + ' ' + str(row['Abstract'])\n",
    "        words = tokenize(text)\n",
    "        tokenized_texts.append(words)\n",
    "    return tokenized_texts\n",
    "\n",
    "# TFIDF (Term frequency and inverse document frequency)\n",
    "\n",
    "def get_word_stat(tokenized_texts):\n",
    "    '''Words counts in documents\n",
    "    finds in how many documents this word \n",
    "    is present\n",
    "    '''\n",
    "    texts_number = len(tokenized_texts)\n",
    "    word2text_count = defaultdict(int)\n",
    "    for text in tokenized_texts:\n",
    "        uniquewords = set(text)\n",
    "        for word in uniquewords:\n",
    "            word2text_count[word] +=1\n",
    "    return word2text_count\n",
    "\n",
    "def get_doc_tfidf(words, word2text_count, N):\n",
    "    num_words = len(words)\n",
    "    word2tfidf = defaultdict(int)\n",
    "    for word in words:\n",
    "        if word2text_count[word] > 0:\n",
    "            idf = np.log(N/(word2text_count[word]))\n",
    "            word2tfidf[word] += (1/num_words) * idf\n",
    "        else:\n",
    "            word2tfidf[word] = 1\n",
    "    return word2tfidf\n",
    "\n",
    "def create_pmi_dict(tokenized_texts, targets, min_count=5):\n",
    "    np.seterr(divide = 'ignore')\n",
    "    \n",
    "    # words count\n",
    "    d = {0:defaultdict(int), 1:defaultdict(int), 'tot':defaultdict(int)}\n",
    "    for idx, words in enumerate(tokenized_texts):\n",
    "        target = targets[idx]\n",
    "        for w in words:\n",
    "            d[ target ][w] += 1\n",
    "            \n",
    "    Dictionary = set(list(d[0].keys()) + list(d[1].keys()))\n",
    "    d['tot'] = {w:d[0][w] + d[1][w] for w in Dictionary}\n",
    "    \n",
    "    # pmi calculation\n",
    "    N_0 = sum(d[0].values())\n",
    "    N_1 = sum(d[1].values())\n",
    "    d[0] = {w: -np.log((v/N_0 + 10**(-15)) / (0.5 * d['tot'][w]/(N_0 + N_1))) / np.log(v/N_0 + 10**(-15))\n",
    "            for w, v in d[0].items() if d['tot'][w] > min_count}\n",
    "    \n",
    "    d[1] = {w: -np.log((v/N_1+ 10**(-15)) / (0.5 * d['tot'][w]/(N_0 + N_1))) / np.log(v/N_1 + 10**(-15))\n",
    "            for w, v in d[1].items() if d['tot'][w] > min_count}\n",
    "    del d['tot']\n",
    "    return d    \n",
    "\n",
    "def classify_pmi_based(words_pmis, word2text_count, tokenized_test_texts, N):\n",
    "    results = np.zeros(len(tokenized_test_texts))\n",
    "    for idx, words in enumerate(tokenized_test_texts):\n",
    "        word2tfidf = get_doc_tfidf(words, word2text_count, N)\n",
    "        # print(word2tfidf)\n",
    "        # PMI - determines significance of the word for the class\n",
    "        # TFIDF - determines significance of the word for the document\n",
    "        tot_pmi0 = [ words_pmis[0][w] * word2tfidf[w] for w in set(words) if w in words_pmis[0] ]\n",
    "        tot_pmi1 = [ words_pmis[1][w] * word2tfidf[w] for w in set(words) if w in words_pmis[1] ]\n",
    "        pmi0 = np.sum(tot_pmi0)\n",
    "        pmi1 = np.sum(tot_pmi1)\n",
    "        diff = pmi1 - pmi0\n",
    "        if diff > 0.006:\n",
    "            results[idx] = 1\n",
    "    return results\n",
    "\n",
    "def text_embeddings(text_tokenized, words_pmis, word2text_count, N):\n",
    "    embeddings = []\n",
    "    for words in tqdm(text_tokenized):\n",
    "        word2tfidf = get_doc_tfidf(words, word2text_count, N)\n",
    "        embedding = torch.FloatTensor(np.zeros( nlp(text_tokenized[0][0]).vector.shape[0] + 2)).to(device)\n",
    "        pmi0 = 0;\n",
    "        pmi1 = 0;\n",
    "        for word in words:\n",
    "            embedding[:200] += torch.FloatTensor(nlp(word).vector).to(device)\n",
    "            try:\n",
    "                pmi0 += words_pmis[0][word] * word2tfidf[word]\n",
    "                pmi1 += words_pmis[1][word] * word2tfidf[word]\n",
    "            except:\n",
    "                continue\n",
    "        embedding[-1] = pmi0\n",
    "        embedding[-2] = pmi1\n",
    "        embeddings.append(embedding / len(words))\n",
    "    return embeddings\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOwtYZgs0UTP"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eFp5ae920UTY",
    "outputId": "dbd5465c-3cac-46b0-ca9b-696843531895"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2841, 5)\n",
      "(11364, 5)\n"
     ]
    }
   ],
   "source": [
    "data = data_raw.sample(frac=1)\n",
    "idx = int(data.shape[0] * 0.2)\n",
    "test_data = data.iloc[:idx]\n",
    "train_data = data.iloc[idx:]\n",
    "print(test_data.shape)\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "uZ1K3V950UTh"
   },
   "outputs": [],
   "source": [
    "tokenized_texts = tokenization(train_data)\n",
    "tokenized_test_texts = tokenization(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11364/11364 [46:08<00:00,  4.11it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46min 12s, sys: 3.38 s, total: 46min 16s\n",
      "Wall time: 46min 9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word2text_count = get_word_stat( tokenized_texts )\n",
    "targets_train = train_data['Label'].values\n",
    "targets_test = test_data['Label'].values \n",
    "N = len(tokenized_texts)\n",
    "words_pmis = create_pmi_dict(tokenized_texts, targets_train, min_count=5)\n",
    "embeddings = text_embeddings(tokenized_texts, words_pmis, word2text_count, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "waeM2sli0UTk",
    "outputId": "507f5608-f398-4892-828b-41d5f5b1e0bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9412178810278071 \n",
      "Precision: 0.9502046384720327\n"
     ]
    }
   ],
   "source": [
    "word2text_count = get_word_stat( tokenized_texts )\n",
    "N = len(tokenized_texts)\n",
    "\n",
    "targets_train = train_data['Label'].values\n",
    "targets_test = test_data['Label'].values\n",
    "\n",
    "words_pmis = create_pmi_dict(tokenized_texts, targets_train, min_count=5)\n",
    "results = classify_pmi_based(words_pmis, word2text_count, tokenized_test_texts, N)\n",
    "\n",
    "precision = np.sum( np.logical_and(results, targets_test) ) / np.sum(targets_test)\n",
    "accuracy = (results == targets_test).mean()\n",
    "\n",
    "print(\"Accuracy: %s \\nPrecision: %s\" % (accuracy, precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network classifier\n",
    "1. NN takes a vector as an input that consists of pmi, tfidf and multiplied embedding for all\n",
    "words in the text <br>\n",
    "2. NN has one hidden layer with N nonlinear neurons <br>\n",
    "3. It has one linear output neuron <br>\n",
    "4. Loss function -- BinaryCrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class net(torch.nn.Module):\n",
    "    def __init__(self, n_hidden_neurons, in_features, out_features):\n",
    "        super(net, self).__init__()\n",
    "        \n",
    "        self.layer1 = torch.nn.Linear(in_features, n_hidden_neurons)\n",
    "        self.act = torch.nn.Sigmoid()\n",
    "        self.layer2 = torch.nn.Linear(n_hidden_neurons, out_features)\n",
    "        self.act_out = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.act_out(x)\n",
    "        return x\n",
    "\n",
    "loss = torch.nn.BCELoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2841/2841 [11:10<00:00,  4.24it/s]\n"
     ]
    }
   ],
   "source": [
    "test_embeddings2 = text_embeddings(tokenized_test_texts, words_pmis, word2text_count, N=N)\n",
    "with open('test_embeddings.p', 'wb') as f:\n",
    "    pickle.dump(test_embeddings2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_embeddings.p', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    test_embeddings3 = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11364/11364 [50:45<00:00,  3.73it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50min 43s, sys: 4.7 s, total: 50min 48s\n",
      "Wall time: 50min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embeddings2 = text_embeddings(tokenized_texts, words_pmis, word2text_count, N)\n",
    "with open('train_embeddings.p', 'wb') as f:\n",
    "    pickle.dump(embeddings2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embeddings_new.p', 'wb') as f:\n",
    "    pickle.dump(embeddings_new, f)\n",
    "with open('test_embeddings_new.p', 'wb') as f:\n",
    "    pickle.dump(test_embeddings_new, f)\n",
    "with open('targets_train.p', 'wb') as f:\n",
    "    pickle.dump(targets_train, f)\n",
    "with open('targets_test.p', 'wb') as f:\n",
    "    pickle.dump(targets_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [1],\n",
       "        [1],\n",
       "        ...,\n",
       "        [1],\n",
       "        [1],\n",
       "        [0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_train = train_data['Label'].values\n",
    "targets_test = test_data['Label'].values\n",
    "targets_train = torch.LongTensor(targets_train)\n",
    "targets_test = torch.LongTensor(targets_test)\n",
    "targets_train.unsqueeze_(1)\n",
    "targets_test.unsqueeze_(1)\n",
    "targets_train.to(device)\n",
    "targets_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_unsq = [e.unsqueeze(0) for e in embeddings2]\n",
    "test_embeddings_unsq = [e.unsqueeze(0) for e in test_embeddings2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_new = torch.cat(embeddings_unsq, dim=0)\n",
    "test_embeddings_new = torch.cat(test_embeddings_unsq, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_new = embeddings_new.to(device)\n",
    "test_embeddings_new = test_embeddings_new.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, X_test, Y_test, batch_size=13, epochs=87):\n",
    "    for epoch in range(epochs):\n",
    "        order = np.random.permutation(len(X))\n",
    "        for start_index in range(0, len(X), batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            batch_indices = order[start_index:start_index+batch_size]\n",
    "            x_batch = X[batch_indices]\n",
    "            y_batch = torch.LongTensor( Y[batch_indices] )\n",
    "            preds = dili_net.forward(x_batch)\n",
    "            \n",
    "            loss_value = loss(preds.float(), y_batch.float())\n",
    "            loss_value.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "        if epoch % 50 == 0:\n",
    "            test_preds = dili_net.forward(X_test)\n",
    "       \n",
    "            test_preds = torch.where(test_preds > 0.3, 1, 0)\n",
    "  \n",
    "            print((test_preds == Y_test).float().mean().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [1],\n",
       "        [1],\n",
       "        ...,\n",
       "        [1],\n",
       "        [1],\n",
       "        [0]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.51601547\n",
      "0.9232665\n",
      "0.9281943\n",
      "0.9334741\n",
      "0.937346\n",
      "0.94156986\n",
      "0.9412179\n",
      "0.9391059\n",
      "0.9391059\n",
      "0.93945795\n",
      "0.9408659\n",
      "0.93805\n",
      "0.937346\n",
      "0.93629\n",
      "0.93593806\n",
      "0.936994\n",
      "0.93875396\n",
      "0.9391059\n",
      "0.93805\n",
      "0.93945795\n",
      "0.9391059\n",
      "0.9408659\n",
      "0.9398099\n",
      "0.937698\n",
      "0.937346\n",
      "0.936994\n",
      "0.9345301\n",
      "0.93593806\n",
      "0.9334741\n",
      "0.93312216\n"
     ]
    }
   ],
   "source": [
    "dili_net = net(5, 202, 1)\n",
    "dili_net.to(device)\n",
    "optimizer = torch.optim.Adam(dili_net.parameters(), \n",
    "                             lr=1.0e-3)\n",
    "train(X=embeddings_new, Y=targets_train, X_test=test_embeddings_new, Y_test=targets_test, batch_size=100, epochs=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rX_E4V4U0UTw",
    "outputId": "77f8a549-ac5e-4700-d1f5-bf33fee288b7"
   },
   "outputs": [],
   "source": [
    "-5.42 / np.log(0.0000000000001)\n",
    "tokenize(\"Hepato and hepatocytes and 8SDiasadfgh;vnjskF bnGIADFBVdvgnk%%SA:F<L\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "classification_DILI.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
